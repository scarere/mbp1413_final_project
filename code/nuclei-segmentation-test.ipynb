{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install required packages\n\n!pip install kornia\n!pip install torchmetrics\n\n# Set some parameters\n\nclass Argument():\n    def __init__(self):\n        self.base_dir = '/kaggle/input/segmentation-dataset/'\n        self.save_dir = '/kaggle/working/'\n        self.IMG_WIDTH = 128\n        self.IMG_HEIGHT = 128\n        self.IMG_CHANNELS = 3\n        self.width_out = 128\n        self.height_out = 128\n        self.batch_size = 32\n        self.learning_rate = 0.01\n        self.epochs = 100\n        self.epoch_lapse = 10\n        self.threshold = 0.33\n        self.sample_size = None\n        self.ishybrid = False\n        self.isAttention = False\n        self.attn_type = 'cosine' # 'cosine', regular_pointwise', 'regular_full', 'regular_full_dim_add' 'channel_attention'\n        self.use_split_data = True\n        self.power = 2\n        self.is_swap_coeffs = False\n        self.is_train_data_aug = False\n        self.is_apply_color_jitter = False\n        self.save_model = True\n\nargs = Argument()","metadata":{"execution":{"iopub.status.busy":"2022-02-18T21:57:10.859899Z","iopub.execute_input":"2022-02-18T21:57:10.860290Z","iopub.status.idle":"2022-02-18T21:57:26.561299Z","shell.execute_reply.started":"2022-02-18T21:57:10.860217Z","shell.execute_reply":"2022-02-18T21:57:26.560509Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nos.chdir(args.base_dir)\nimport sys\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom itertools import chain\nfrom sklearn.model_selection import train_test_split\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.morphology import label\nimport torch\n\nfrom loss_utils import *\nfrom model_utils import *\nfrom dataset_train_val import Dataset_train_val\nfrom training_validation_utils import *\nfrom plotutils import *\n\n\nwarnings.filterwarnings('ignore', category=UserWarning, module='skimage')\nseed = 42\nrandom.seed = seed\nnp.random.seed = seed\n\nuse_gpu = torch.cuda.is_available()\ndtv = Dataset_train_val(args.batch_size, use_gpu)\n\ndef create_model_name(args):\n    if args.ishybrid == True:\n        if args.is_swap_coeffs == True:\n            model_name = 'unet_hybrid_power'+str(args.power)+'_swap_coeffs'\n        else:\n            model_name = 'unet_hybrid_power'+str(args.power)\n    else:\n        if args.isAttention == True:\n            model_name = 'unet_'+args.attn_type+'_attn'\n        else:\n            model_name = 'unet_no_attn'\n    if args.is_train_data_aug == True:\n        model_name = model_name + '_augmentation'\n    return model_name + '.pt'\n\nif args.use_split_data == True:\n    x_train = torch.load('x_train_split.pt')\n    x_val = torch.load('x_val_split.pt')\n    y_train = torch.load('y_train_split.pt')\n    y_val = torch.load('y_val_split.pt')\n    x_test = torch.load('x_test.pt')\n\nelse:\n    x_train, x_val, y_train, y_val, x_test = dtv.load_train_val_test('x_train.pt', 'y_train.pt', 'x_test.pt', split_ratio=0.2)\n# print(x_train.shape,x_val.shape,x_test.shape)\n\n# Train Model\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom tqdm import trange\nfrom time import sleep\n\nif args.ishybrid == True:\n    print('Training Hybrid Net')\n    flatten_arg = False # for hybrid net\n    unet = Hybrid_Net(img_ch=3,output_ch=1) \n    criterion = DiceBCELossModified(full_flatten=flatten_arg) # nn.CrossEntropyLoss() # TverskyBCELoss()\nelse:\n    if args.isAttention == True:\n        print('Training Attention U-Net')\n        unet = AttU_Net(img_ch=3,output_ch=1, attn_type = args.attn_type)\n    else:\n        print('Training Vanilla U-Net')\n        unet = UNet(in_channel=3,out_channel=1)\n    criterion = DiceBCELoss() # nn.CrossEntropyLoss() # TverskyBCELoss()\n\nif use_gpu:\n    unet = unet.cuda()\noptimizer = optim.SGD(unet.parameters(), lr = args.learning_rate, momentum=0.99)\ntrain_losses, val_losses = Training_Validation(ishybrid=args.ishybrid,power=args.power,swap_coeffs=args.is_swap_coeffs,is_train_data_aug=args.is_train_data_aug,is_apply_color_jitter=args.is_apply_color_jitter).train_valid(unet, x_train , y_train, x_val, y_val, optimizer, criterion, args.batch_size, dtv, use_gpu, epochs = args.epochs, epoch_lapse = args.epoch_lapse)\n\nif args.save_model == True:\n    torch.save(unet,args.save_dir+create_model_name(args))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-18T21:57:54.396724Z","iopub.execute_input":"2022-02-18T21:57:54.397041Z","iopub.status.idle":"2022-02-18T22:10:15.992715Z","shell.execute_reply.started":"2022-02-18T21:57:54.397001Z","shell.execute_reply":"2022-02-18T22:10:15.992018Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"**Train Model**","metadata":{}},{"cell_type":"markdown","source":"**Plot Results**","metadata":{}},{"cell_type":"code","source":"allp = Allplots()\n\nif args.ishybrid == True:\n    allp.plot_losses(train_losses, [])\nelse:\n    allp.plot_losses(train_losses, val_losses)\n\nallp.plot_examples(unet, x_train, y_train, 12, ishybrid=args.ishybrid)\n\nallp.plot_examples(unet, x_val, y_val, 12, ishybrid=args.ishybrid)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T22:10:15.994397Z","iopub.execute_input":"2022-02-18T22:10:15.995102Z","iopub.status.idle":"2022-02-18T22:10:24.047872Z","shell.execute_reply.started":"2022-02-18T22:10:15.995062Z","shell.execute_reply":"2022-02-18T22:10:24.047272Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"**Display Metrics**","metadata":{}},{"cell_type":"code","source":"from segment_metrics import IOU_eval\n\niou_ev = IOU_eval(ishybrid=args.ishybrid)\n\nprint('Training')\niou_t, iou_t_indices = iou_ev.iou_evaluate(unet, x_train, y_train)\nallp.plot_best(unet, x_train, datay=y_train, indx=np.argsort(iou_t)[-5:], index_ranks=iou_t_indices, ishybrid=args.ishybrid)\n\nprint('Validation')\niou_v, iou_v_indices = iou_ev.iou_evaluate(unet, x_val, y_val)\nallp.plot_best(unet, x_val, datay=y_val, indx=np.argsort(iou_v)[-5:], index_ranks=iou_v_indices, ishybrid=args.ishybrid)\n\nprint('Testing')\nallp.plot_best(unet, x_test, datay=None, indx=np.random.permutation(65)[:5], index_ranks=np.zeros(5), ishybrid=args.ishybrid)\n\nprint('Train Mean IOU: '+str(np.mean(iou_t))+', Valid Mean IOU: '+str(np.mean(iou_v)))","metadata":{"execution":{"iopub.status.busy":"2022-02-18T22:10:24.049383Z","iopub.execute_input":"2022-02-18T22:10:24.049834Z","iopub.status.idle":"2022-02-18T22:10:35.116570Z","shell.execute_reply.started":"2022-02-18T22:10:24.049798Z","shell.execute_reply":"2022-02-18T22:10:35.115785Z"},"trusted":true},"execution_count":6,"outputs":[]}]}